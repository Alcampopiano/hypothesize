{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Hypothesize: robust statistics in Python \u00b6 Hypothesize is a robust statistics library for Python based on Rand R. Wilcox's R package WRS . With Hypothesize you can compare groups and measure associations using methods that outperform traditional statistcal approaches in terms of power and accuracy. For more information on robust methods please see Wilcox's book Introduction to Robust Estimation and Hypothesis Testing . Getting Started \u00b6 Overview Installation Basic Tutorial User Guide \u00b6 Function reference Frequently asked questions Bug reports and Questions \u00b6 Hypothesize is BSD-licenced and the source code is available on GitHub . For issues and questions, please use GitHub Issues .","title":"Home"},{"location":"#hypothesize-robust-statistics-in-python","text":"Hypothesize is a robust statistics library for Python based on Rand R. Wilcox's R package WRS . With Hypothesize you can compare groups and measure associations using methods that outperform traditional statistcal approaches in terms of power and accuracy. For more information on robust methods please see Wilcox's book Introduction to Robust Estimation and Hypothesis Testing .","title":"Hypothesize: robust statistics in Python"},{"location":"#getting-started","text":"Overview Installation Basic Tutorial","title":"Getting Started"},{"location":"#user-guide","text":"Function reference Frequently asked questions","title":"User Guide"},{"location":"#bug-reports-and-questions","text":"Hypothesize is BSD-licenced and the source code is available on GitHub . For issues and questions, please use GitHub Issues .","title":"Bug reports and Questions"},{"location":"FAQ/","text":"Frequently asked questions \u00b6 No attempt is made to fully explain the following concepts, but hopefully this gets you started. The Internet has plenty of resources on these topics if you would like to learn more. What is a trimmed mean? \u00b6 The trimmed mean involves calculating the sample mean after removing a proportion of values from each tail of the distribution. In symbols the trimmed mean is expressed as follows: \\bar{X}_t = \\frac{X_{(g+1)}\\,+,...,+\\,X_{(n-g)}}{n-2g} \\bar{X}_t = \\frac{X_{(g+1)}\\,+,...,+\\,X_{(n-g)}}{n-2g} where X_1, \\,X_2,\\,...\\,,X_n X_1, \\,X_2,\\,...\\,,X_n is a random sample and X_{(1)}, \\le X_{(2)}\\,,...,\\,\\le X_{(n)} X_{(1)}, \\le X_{(2)}\\,,...,\\,\\le X_{(n)} are the observations in ascending order. The proportion to trim is \\gamma\\,(0\\lt \\gamma \\lt.5) \\gamma\\,(0\\lt \\gamma \\lt.5) and g = [ \\gamma n ] g = [ \\gamma n ] rounded down to the nearest integer. What is bootstrapping? \u00b6 In the context of hypothesis testing and generally speaking, bootstrapping involves taking many random samples (with replacement) from the data at hand in order to estimate a sampling distribution of interest. This is in contrast to traditional methods which assume the shape of the particular sampling distribution under study. Once we have an emprically derived sampling distribution, obtaining CIs and p values is relatively straightforward. What is a contrast matrix? \u00b6 First, it is helpful to imagine your design arranged into a JxK matrix. A=\\begin{bmatrix} a_{1,1} & a_{1,2} & ... & a_{1,K} \\\\ a_{2,1} & a_{2,2} & ... & a_{2,K} \\\\ a_{J,1} & a_{J,2} & ... & a_{J,K} \\end{bmatrix} A=\\begin{bmatrix} a_{1,1} & a_{1,2} & ... & a_{1,K} \\\\ a_{2,1} & a_{2,2} & ... & a_{2,K} \\\\ a_{J,1} & a_{J,2} & ... & a_{J,K} \\end{bmatrix} A contrast matrix specifies which cells (or elements) in the above design are to be compared. The rows in a contrast matrix correspond to the cells in your design. The columns correspond to the contrasts that you wish to make. Examples of contrast matrices for different designs \u00b6 Matrix notation is used to explain which cells are being compared, followed by the corresponding contrast matrix. design with 2 groups {a_{1,1} - a_{1,2}} {a_{1,1} - a_{1,2}} contrast 1 1 -1 design with 3 groups \\Large{a_{1,1} - a_{1,2}} \\Large{a_{1,1} - a_{1,2}} \\Large{a_{1,1} - a_{1,3}} \\Large{a_{1,1} - a_{1,3}} \\Large{a_{1,2} - a_{1,3}} \\Large{a_{1,2} - a_{1,3}} contrast 1 contrast 2 contrast 3 1 1 0 -1 0 1 0 -1 -1 2x2 design Factor A \\Large{(a_{1,1} + a_{1,2})-(a_{2,1} + a_{2,2})} \\Large{(a_{1,1} + a_{1,2})-(a_{2,1} + a_{2,2})} contrast 1 1 1 -1 -1 Factor B \\Large{(a_{1,1} + a_{2,1})-(a_{1,2} + a_{2,2})} \\Large{(a_{1,1} + a_{2,1})-(a_{1,2} + a_{2,2})} contrast 1 1 -1 1 -1 Interaction \\Large{(a_{1,1} + a_{2,2})-(a_{1,2} + a_{2,1})} \\Large{(a_{1,1} + a_{2,2})-(a_{1,2} + a_{2,1})} That is, the difference of the differences contrast 1 1 -1 -1 1 2x3 design Factor A \\Large{(a_{1,1} + a_{1,2} + a_{1,3})-(a_{2,1} + a_{2,2} + a_{2,3})} \\Large{(a_{1,1} + a_{1,2} + a_{1,3})-(a_{2,1} + a_{2,2} + a_{2,3})} contrast 1 1 1 1 -1 -1 -1 Factor B \\Large{(a_{1,1} + a_{2,1})-(a_{1,2} + a_{2,2})} \\Large{(a_{1,1} + a_{2,1})-(a_{1,2} + a_{2,2})} \\Large{(a_{1,1} + a_{2,1})-(a_{1,3} + a_{2,3})} \\Large{(a_{1,1} + a_{2,1})-(a_{1,3} + a_{2,3})} \\Large{(a_{1,2} + a_{2,2})-(a_{1,3} + a_{2,3})} \\Large{(a_{1,2} + a_{2,2})-(a_{1,3} + a_{2,3})} contrast 1 contrast 2 contrast 3 1 1 0 -1 0 1 0 -1 -1 1 1 0 -1 0 1 0 -1 -1 Interactions \\Large{(a_{1,1} + a_{2,2})-(a_{1,2} + a_{2,1})} \\Large{(a_{1,1} + a_{2,2})-(a_{1,2} + a_{2,1})} \\Large{(a_{1,1} + a_{2,3})-(a_{1,3} + a_{2,1})} \\Large{(a_{1,1} + a_{2,3})-(a_{1,3} + a_{2,1})} \\Large{(a_{1,2} + a_{2,3})-(a_{1,3} + a_{2,2})} \\Large{(a_{1,2} + a_{2,3})-(a_{1,3} + a_{2,2})} contrast 1 contrast 2 contrast 3 1 1 0 -1 0 1 0 -1 -1 -1 -1 0 1 0 -1 0 1 1 Not a fan of contrast matrices? Don't worry, Hypothesize can generate all linear contrasts automatically (see functions con1Way and con2way ). However, it is useful to understand this concept so that you know which comparisons are being made and how to specify your own if necessary.","title":"FAQ"},{"location":"FAQ/#frequently-asked-questions","text":"No attempt is made to fully explain the following concepts, but hopefully this gets you started. The Internet has plenty of resources on these topics if you would like to learn more.","title":"Frequently asked questions"},{"location":"FAQ/#what-is-a-trimmed-mean","text":"The trimmed mean involves calculating the sample mean after removing a proportion of values from each tail of the distribution. In symbols the trimmed mean is expressed as follows: \\bar{X}_t = \\frac{X_{(g+1)}\\,+,...,+\\,X_{(n-g)}}{n-2g} \\bar{X}_t = \\frac{X_{(g+1)}\\,+,...,+\\,X_{(n-g)}}{n-2g} where X_1, \\,X_2,\\,...\\,,X_n X_1, \\,X_2,\\,...\\,,X_n is a random sample and X_{(1)}, \\le X_{(2)}\\,,...,\\,\\le X_{(n)} X_{(1)}, \\le X_{(2)}\\,,...,\\,\\le X_{(n)} are the observations in ascending order. The proportion to trim is \\gamma\\,(0\\lt \\gamma \\lt.5) \\gamma\\,(0\\lt \\gamma \\lt.5) and g = [ \\gamma n ] g = [ \\gamma n ] rounded down to the nearest integer.","title":"What is a trimmed mean?"},{"location":"FAQ/#what-is-bootstrapping","text":"In the context of hypothesis testing and generally speaking, bootstrapping involves taking many random samples (with replacement) from the data at hand in order to estimate a sampling distribution of interest. This is in contrast to traditional methods which assume the shape of the particular sampling distribution under study. Once we have an emprically derived sampling distribution, obtaining CIs and p values is relatively straightforward.","title":"What is bootstrapping?"},{"location":"FAQ/#what-is-a-contrast-matrix","text":"First, it is helpful to imagine your design arranged into a JxK matrix. A=\\begin{bmatrix} a_{1,1} & a_{1,2} & ... & a_{1,K} \\\\ a_{2,1} & a_{2,2} & ... & a_{2,K} \\\\ a_{J,1} & a_{J,2} & ... & a_{J,K} \\end{bmatrix} A=\\begin{bmatrix} a_{1,1} & a_{1,2} & ... & a_{1,K} \\\\ a_{2,1} & a_{2,2} & ... & a_{2,K} \\\\ a_{J,1} & a_{J,2} & ... & a_{J,K} \\end{bmatrix} A contrast matrix specifies which cells (or elements) in the above design are to be compared. The rows in a contrast matrix correspond to the cells in your design. The columns correspond to the contrasts that you wish to make.","title":"What is a contrast matrix?"},{"location":"FAQ/#examples-of-contrast-matrices-for-different-designs","text":"Matrix notation is used to explain which cells are being compared, followed by the corresponding contrast matrix. design with 2 groups {a_{1,1} - a_{1,2}} {a_{1,1} - a_{1,2}} contrast 1 1 -1 design with 3 groups \\Large{a_{1,1} - a_{1,2}} \\Large{a_{1,1} - a_{1,2}} \\Large{a_{1,1} - a_{1,3}} \\Large{a_{1,1} - a_{1,3}} \\Large{a_{1,2} - a_{1,3}} \\Large{a_{1,2} - a_{1,3}} contrast 1 contrast 2 contrast 3 1 1 0 -1 0 1 0 -1 -1 2x2 design Factor A \\Large{(a_{1,1} + a_{1,2})-(a_{2,1} + a_{2,2})} \\Large{(a_{1,1} + a_{1,2})-(a_{2,1} + a_{2,2})} contrast 1 1 1 -1 -1 Factor B \\Large{(a_{1,1} + a_{2,1})-(a_{1,2} + a_{2,2})} \\Large{(a_{1,1} + a_{2,1})-(a_{1,2} + a_{2,2})} contrast 1 1 -1 1 -1 Interaction \\Large{(a_{1,1} + a_{2,2})-(a_{1,2} + a_{2,1})} \\Large{(a_{1,1} + a_{2,2})-(a_{1,2} + a_{2,1})} That is, the difference of the differences contrast 1 1 -1 -1 1 2x3 design Factor A \\Large{(a_{1,1} + a_{1,2} + a_{1,3})-(a_{2,1} + a_{2,2} + a_{2,3})} \\Large{(a_{1,1} + a_{1,2} + a_{1,3})-(a_{2,1} + a_{2,2} + a_{2,3})} contrast 1 1 1 1 -1 -1 -1 Factor B \\Large{(a_{1,1} + a_{2,1})-(a_{1,2} + a_{2,2})} \\Large{(a_{1,1} + a_{2,1})-(a_{1,2} + a_{2,2})} \\Large{(a_{1,1} + a_{2,1})-(a_{1,3} + a_{2,3})} \\Large{(a_{1,1} + a_{2,1})-(a_{1,3} + a_{2,3})} \\Large{(a_{1,2} + a_{2,2})-(a_{1,3} + a_{2,3})} \\Large{(a_{1,2} + a_{2,2})-(a_{1,3} + a_{2,3})} contrast 1 contrast 2 contrast 3 1 1 0 -1 0 1 0 -1 -1 1 1 0 -1 0 1 0 -1 -1 Interactions \\Large{(a_{1,1} + a_{2,2})-(a_{1,2} + a_{2,1})} \\Large{(a_{1,1} + a_{2,2})-(a_{1,2} + a_{2,1})} \\Large{(a_{1,1} + a_{2,3})-(a_{1,3} + a_{2,1})} \\Large{(a_{1,1} + a_{2,3})-(a_{1,3} + a_{2,1})} \\Large{(a_{1,2} + a_{2,3})-(a_{1,3} + a_{2,2})} \\Large{(a_{1,2} + a_{2,3})-(a_{1,3} + a_{2,2})} contrast 1 contrast 2 contrast 3 1 1 0 -1 0 1 0 -1 -1 -1 -1 0 1 0 -1 0 1 1 Not a fan of contrast matrices? Don't worry, Hypothesize can generate all linear contrasts automatically (see functions con1Way and con2way ). However, it is useful to understand this concept so that you know which comparisons are being made and how to specify your own if necessary.","title":"Examples of contrast matrices for different designs"},{"location":"basic_tutorial/","text":"Basic Tutorial \u00b6 The following tutorial demonstrates how to perform a robust hypothesis test using 20% trimmed means and the bootstrap-t test. The data correspond to a 2 (between-subjects) x 3 (within-subjects) factorial design. Getting your data into Hypothesize \u00b6 In Hypothesize, input data are always specified as a Pandas DataFrame or Series. In this example, we have a 2x3 factorial design so the data would take the form of a six-column DataFrame (i.e., J levels x K levels). Using Pandas you can read your data into Python and use one of the appropriate functions from Hypothesize. In this case we will use the function bwmcp but there are many others to choose from. \"What about my column names?\" Don't worry, Hypothesize doesn't make use of your column names. Feel free to name them however you like! import pandas as pd df = pd . read_csv ( 'my_data.csv' ) df . head () cell_1_1 cell_1_2 cell_1_3 cell_2_1 cell_2_2 cell_2_3 0.04 0.90 0.79 0.51 0.33 0.23 0.76 0.29 0.84 0.03 0.5 0.73 0.71 0.59 0.11 0.89 0.76 0.04 0.17 0.26 0.88 0.28 0.1 0.21 0.95 0.22 0.83 0.59 0.65 0.20 from hypothesize.compare_groups_with_two_factors import bwmcp results = bwmcp ( J = 2 , K = 3 , x = df ) Examining your results \u00b6 The results are returned as a Python Dictionary containing simple Python objects or DataFrames (when the results are best given as a matrix). For example, here are the previously computed results for the interaction returned as a DataFrame. results [ 'factor_AB' ] con_num psihat se test crit_value p_value 0 -0.100698 0.126135 -0.798336 2.3771 0.410684 1 -0.037972 0.151841 -0.250078 2.3771 0.804674 2 0.0627261 0.135392 0.463291 2.3771 0.659432 Try this example yourself in Colab!","title":"Tutorial"},{"location":"basic_tutorial/#basic-tutorial","text":"The following tutorial demonstrates how to perform a robust hypothesis test using 20% trimmed means and the bootstrap-t test. The data correspond to a 2 (between-subjects) x 3 (within-subjects) factorial design.","title":"Basic Tutorial"},{"location":"basic_tutorial/#getting-your-data-into-hypothesize","text":"In Hypothesize, input data are always specified as a Pandas DataFrame or Series. In this example, we have a 2x3 factorial design so the data would take the form of a six-column DataFrame (i.e., J levels x K levels). Using Pandas you can read your data into Python and use one of the appropriate functions from Hypothesize. In this case we will use the function bwmcp but there are many others to choose from. \"What about my column names?\" Don't worry, Hypothesize doesn't make use of your column names. Feel free to name them however you like! import pandas as pd df = pd . read_csv ( 'my_data.csv' ) df . head () cell_1_1 cell_1_2 cell_1_3 cell_2_1 cell_2_2 cell_2_3 0.04 0.90 0.79 0.51 0.33 0.23 0.76 0.29 0.84 0.03 0.5 0.73 0.71 0.59 0.11 0.89 0.76 0.04 0.17 0.26 0.88 0.28 0.1 0.21 0.95 0.22 0.83 0.59 0.65 0.20 from hypothesize.compare_groups_with_two_factors import bwmcp results = bwmcp ( J = 2 , K = 3 , x = df )","title":"Getting your data into Hypothesize"},{"location":"basic_tutorial/#examining-your-results","text":"The results are returned as a Python Dictionary containing simple Python objects or DataFrames (when the results are best given as a matrix). For example, here are the previously computed results for the interaction returned as a DataFrame. results [ 'factor_AB' ] con_num psihat se test crit_value p_value 0 -0.100698 0.126135 -0.798336 2.3771 0.410684 1 -0.037972 0.151841 -0.250078 2.3771 0.804674 2 0.0627261 0.135392 0.463291 2.3771 0.659432 Try this example yourself in Colab!","title":"Examining your results"},{"location":"function_guide/","text":"Function Reference \u00b6 Hypothesize exposes the following top-level functions for comparing groups and measuring associations. The function names, code, and descriptions are kept generally consistent with Wilcox's WRS package. If you want to learn more about the theory and research behind any given function here, see Wilcox's books, especially Introduction to Robust Estimation and Hypothesis Testing . Jump to: Comparing groups with a single factor Comparing groups with two factors Measuring associations Comparing groups with a single factor \u00b6 Statistical tests analogous to a 1-way ANOVA or T-tests. That is, group tests that have a single factor. Independent groups \u00b6 l2drmci \u00b6 l2drmci ( x , y , est , * args , pairwise_drop_na = True , alpha =. 05 , nboot = 2000 , seed = False ) Compute a bootstrap confidence interval for a measure of location associated with the distribution of x-y. That is, compare x and y by looking at all possible difference scores in random samples of x and y . x and y are possibly dependent. Parameters: x: Pandas Series Data for group one y: Pandas Series Data for group two est: function Measure of location (currently only trim_mean is supported) *args: list/value Parameter(s) for measure of location (e.g., .2) pairwise_drop_na: bool If True, treat data as dependent and remove any row with missing data. If False, remove missing data for each group seperately (cannot deal with unequal sample sizes) alpha: float Alpha level (default is .05) nboot: int Number of bootstrap samples (default is 2000) seed: bool Random seed for reprodicible results (default is False ) Return: Dictonary of results Try this example yourself in Colab! linconb \u00b6 linconb ( x , con , tr =. 2 , alpha =. 05 , nboot = 599 , seed = False ) Compute a 1-alpha confidence interval for a set of d linear contrasts involving trimmed means using the bootstrap-t bootstrap method. Independent groups are assumed. CIs are adjusted to control FWE (p values are not adjusted). Parameters: x: DataFrame Each column represents a group of data con: array con is a J (number of columns) by d (number of contrasts) matrix containing the contrast coefficents of interest. All linear constrasts can be created automatically by using the function con1way (the result of which can be used for con ). tr: float Proportion to trim (default is .2) alpha: float Alpha level (default is .05) nboot: int Number of bootstrap samples (default is 2000) seed: bool Random seed for reprodicible results. Default is False . Return: Dictonary of results Try this example yourself in Colab! pb2gen \u00b6 pb2gen ( x , y , est , * args , alpha =. 05 , nboot = 2000 , seed = False ) Compute a bootstrap confidence interval for the the difference between any two parameters corresponding to two independent groups. Parameters: x: Pandas Series Data for group one y: Pandas Series Data for group two est: function Measure of location (currently only trim_mean is supported) *args: list/value Parameter(s) for measure of location (e.g., .2) alpha: float Alpha level (default is .05) nboot: int Number of bootstrap samples (default is 2000) seed: bool Random seed for reprodicible results (default is False ) Return: Dictonary of results Try this example yourself in Colab! tmcppb \u00b6 tmcppb ( x , est , * args , con = None , bhop = False , alpha =. 05 , nboot = None , seed = False ) Multiple comparisons for J independent groups using trimmed means and the percentile bootstrap method. Rom\u2019s method is used to control the probability of one or more type I errors. For C > 10 hypotheses, or when the goal is to test at some level other than .05 and .01, Hochberg\u2019s method is used. Setting the argument bhop to True uses the Benjamini\u2013Hochberg method instead. Parameters: x: Pandas DataFrame Each column represents a group of data est: function Measure of location (currently only trim_mean is supported) *args: list/value Parameter(s) for measure of location (e.g., .2) con: array con is a J (number of columns) by d (number of contrasts) matrix containing the contrast coefficents of interest. All linear constrasts can be created automatically by using the function con1way (the result of which can be used for con ). The default is None and in this case all linear contrasts are created automatically. bhop: bool If True , the Benjamini\u2013Hochberg method is used to control FWE alpha: float Alpha level. Default is .05. nboot: int Number of bootstrap samples (default is 2000) seed: bool Random seed for reproducible results. Default is False . Return: Dictonary of results Try this example yourself in Colab! yuenbt \u00b6 yuenbt ( x , y , tr =. 2 , alpha =. 05 , nboot = 599 , seed = False ) Compute a 1-alpha confidence interval for the difference between the trimmed means corresponding to two independent groups. The bootstrap-t method is used. During the bootstrapping, the absolute value of the test statistic is used (the \"two-sided method\"). Parameters: x: Pandas Series Data for group one y: Pandas Series Data for group two tr: float Proportion to trim (default is .2) alpha: float Alpha level (default is .05) nboot: int Number of bootstrap samples (default is 599) seed: bool Random seed for reprodicible results. Default is False . Return: Dictonary of results Try this example yourself in Colab! Dependent groups \u00b6 bootdpci \u00b6 bootdpci ( x , est , * args , nboot = None , alpha =. 05 , dif = True , BA = False , SR = False ) Use percentile bootstrap method, compute a .95 confidence interval for the difference between a measure of location or scale when comparing two dependent groups. The argument dif defaults to True indicating that difference scores will be used, in which case Hochberg\u2019s method is used to control FWE. If dif is False , measures of location associated with the marginal distributions are used instead. If dif is False and BA is True , the bias adjusted estimate of the generalized p-value is recommended. Using BA = True (when dif = False ) is recommended when comparing groups with M-estimators and MOM, but it is not necessary when comparing 20% trimmed means (Wilcox & Keselman, 2002). The so-called the SR method, which is a slight modification of Hochberg's (1988) \"sequentially rejective\" method can be applied to control FWE, especially when comparing one-step M-estimators or M-estimators. Parameters: x: Pandas DataFrame Each column represents a group of data est: function Measure of location (currently only trim_mean is supported) *args: list/value Parameter(s) for measure of location (e.g., .2) alpha: float Alpha level. Default is .05. nboot: int Number of bootstrap samples. Default is None in which case nboot will be chosen for you based on the number of contrasts. dif: bool When True , use difference scores, otherwise use marginal distributions BA: bool When True , use the bias adjusted estimate of the generalized p-value is applied (e.g., when dif is False ) SR: bool When True , use the modified \"sequentially rejective\", especially when comparing one-step M-estimators or M-estimators. Return: Dictonary of results Try this example yourself in Colab! rmmcppb \u00b6 rmmcppb ( x , est , * args , alpha =. 05 , con = None , dif = True , nboot = None , BA = False , hoch = False , SR = False , seed = False ) Use a percentile bootstrap method to compare dependent groups. By default, compute a .95 confidence interval for all linear contrasts specified by con, a J-by-C matrix, where C is the number of contrasts to be tested, and the columns of con are the contrast coefficients. If con is not specified, all pairwise comparisons are done. If est is the function onestep or mom (these are not implemeted yet), method SR can be used to control the probability of at least one Type I error. Otherwise, Hochberg's method is used. If dif is False and BA is True , the bias adjusted estimate of the generalized p-value is recommended. Using BA = True (when dif = False ) is recommended when comparing groups with M-estimators and MOM, but it is not necessary when comparing 20% trimmed means (Wilcox & Keselman, 2002). Hochberg's sequentially rejective method can be used and is used if n>=80. Parameters: x: Pandas DataFrame Each column represents a group of data est: function Measure of location (currently only trim_mean is supported) *args: list/value Parameter(s) for measure of location (e.g., .2) alpha: float Alpha level (default is .05) con: array con is a J (number of columns) by d (number of contrasts) matrix containing the contrast coefficents of interest. All linear constrasts can be created automatically by using the function con1way (the result of which can be used for con ). The default is None and in this case all linear contrasts are created automatically. dif: bool When True , use difference scores, otherwise use marginal distributions nboot: int Number of bootstrap samples. Default is None in which case nboot will be chosen for you based on the number of contrasts. BA: bool When True , use the bias adjusted estimate of the generalized p-value is applied (e.g., when dif is False ) hoch: bool When True , Hochberg's sequentially rejective method can be used and is used if n>=80. SR: bool When True , use the modified \"sequentially rejective\", especially when comparing one-step M-estimators or M-estimators. seed: bool Random seed for reprodicible results (default is False ) Return: Dictonary of results Try this example yourself in Colab! lindepbt \u00b6 lindepbt ( x , tr =. 2 , con = None , alpha =. 05 , nboot = 599 , dif = True , seed = False ) Multiple comparisons on trimmed means with FWE controlled with Rom's method Using a bootstrap-t method. Parameters: x: Pandas DataFrame Each column in the data represents a different group tr: float Proportion to trim (default is .2) con: array con is a J (number of groups) by d (number of contrasts) matrix containing the contrast coefficents of interest. All linear constrasts can be created automatically by using the function con1way (the result of which can be used for con ). The default is None and in this case all linear contrasts are created automatically. alpha: float Alpha level. Default is .05. nboot: int Number of bootstrap samples (default is 2000) dif: bool When True , use difference scores, otherwise use marginal distributions seed: bool Random seed for reprodicible results (default is False ) Return: Dictonary of results Try this example yourself in Colab! ydbt \u00b6 ydbt ( x , y , tr =. 2 , alpha =. 05 , nboot = 599 , side = True , seed = False ) Using the bootstrap-t method, compute a .95 confidence interval for the difference between the marginal trimmed means of paired data. By default, 20% trimming is used with 599 bootstrap samples. Parameters: x: Pandas Series Data for group one y: Pandas Series Data for group two tr: float Proportion to trim (default is .2) alpha: float Alpha level. Default is .05. nboot: int Number of bootstrap samples (default is 2000) side: bool When True the function returns a symmetric CI and a p value, otherwise the function returns equal-tailed CI (no p value) seed: bool Random seed for reprodicible results (default is False ) Return: Dictonary of results Try this example yourself in Colab! Comparing groups with two factors \u00b6 Statistical tests analogous to a 2-way ANOVA. That is, group tests that have a two factors. Dependent groups \u00b6 wwmcppb \u00b6 wwmcppb ( J , K , x , est , * args , alpha =. 05 , dif = True , nboot = None , BA = True , hoch = True , seed = False ) Do all multiple comparisons for a within-by-within design using a percentile bootstrap method.A sequentially rejective method is used to control alpha. Hochberg's method can be used and is if n>=80. Parameters: J: int Number of J levels associated with Factor A K: int Number of K levels associated with Factor B x: Pandas DataFrame Each column represents a cell in the factorial design. For example, a 2x3 design would correspond to a DataFrame with 6 columns (levels of Factor A x levels of Factor B). Order your columns according to the following pattern (traversing each row in a matrix): the first column contains data for level 1 of Factor A and level 1 of Factor B the second column contains data for level 1 of Factor A and level 2 of Factor B column K contains the data for level 1 of Factor A and level K of Factor B column K + 1 contains the data for level 2 of Factor A and level 1 of Factor B and so on ... est: function Measure of location (currently only trim_mean is supported) *args: list/value Parameter(s) for measure of location (e.g., .2) alpha: float Alpha level (default is .05) dif: bool When True , use difference scores, otherwise use marginal distributions nboot: int Number of bootstrap samples (default is 599) BA: bool When True , use the bias adjusted estimate of the generalized p-value is applied (e.g., when dif is False ) hoch: bool When True , Hochberg's sequentially rejective method can be used to control FWE seed: bool Random seed for reprodicible results (default is False ) Return: Dictonary of results Try this example yourself in Colab! wwmcpbt \u00b6 wwmcpbt ( J , K , x , tr =. 2 , alpha =. 05 , nboot = 599 , seed = False ) Do multiple comparisons for a within-by-within design. using a bootstrap-t method and trimmed means. All linear contrasts relevant to main effects and interactions are tested. With trimmed means FWE is controlled with Rom's method. (currently unsure whether or not dif should be here) Parameters: J: int Number of J levels associated with Factor A K: int Number of K levels associated with Factor B x: Pandas DataFrame Each column represents a cell in the factorial design. For example, a 2x3 design would correspond to a DataFrame with 6 columns (levels of Factor A x levels of Factor B). Order your columns according to the following pattern (traversing each row in a matrix): the first column contains data for level 1 of Factor A and level 1 of Factor B the second column contains data for level 1 of Factor A and level 2 of Factor B column K contains the data for level 1 of Factor A and level K of Factor B column K + 1 contains the data for level 2 of Factor A and level 1 of Factor B and so on ... tr: float Proportion to trim (default is .2) alpha: float Alpha level (default is .05) nboot: int Number of bootstrap samples (default is 599) seed: bool Random seed for reprodicible results (default is False ) Return: Dictonary of results Try this example yourself in Colab! Mixed designs \u00b6 These designs are also known as \"split-plot\" or \"between-within\" desgins. Hypothesize follws the common convention that assumes that the between-subjects factor is factor A and the within-subjects conditions are Factor B. For example, in a 2x3 mixed design, factor A has two levels. For each of these levels, there are 3 within-subjects conditions. Make sure your DataFrame corresponds to a between-within design, not the other way around For example, in a 2x3 mixed design, the first 3 columns correspond to the first level of Factor A. The last 3 columns correspond to the second level of factor A. bwamcp \u00b6 bwamcp ( J , K , x , tr =. 2 , alpha =. 05 , pool = False ) All pairwise comparisons among levels of Factor A in a mixed design using trimmed means. The pool option allows you to pool dependent groups across Factor A for each level of Factor B. Parameters: J: int Number of J levels associated with Factor A K: int Number of K levels associated with Factor B x: Pandas DataFrame Each column represents a cell in the factorial design. For example, a 2x3 design would correspond to a DataFrame with 6 columns (levels of Factor A x levels of Factor B). Order your columns according to the following pattern (traversing each row in a matrix): the first column contains data for level 1 of Factor A and level 1 of Factor B the second column contains data for level 1 of Factor A and level 2 of Factor B column K contains the data for level 1 of Factor A and level K of Factor B column K + 1 contains the data for level 2 of Factor A and level 1 of Factor B and so on ... tr: float Proportion to trim (default is .2) alpha: float Alpha level (default is .05) pool: bool If True , pool dependent groups together (default is False ). Otherwise generate pairwise contrasts across factor A for each level of factor B. Return: Dictonary of results Try this example yourself in Colab! bwbmcp \u00b6 bwbmcp ( J , K , x , tr =. 2 , con = None , alpha =. 05 , dif = True , pool = False , hoch = False ) All pairwise comparisons among levels of Factor B in a mixed design using trimmed means. The pool option allows you to pool dependent groups across Factor A for each level of Factor B. Rom's method is used to control for FWE (when alpha is 0.5, .01, or when number of comparisons are > 10). Hochberg's method can also be used. Note that CIs are adjusted based on the corresponding critical p-value after controling for FWE. Parameters: J: int Number of J levels associated with Factor A K: int Number of K levels associated with Factor B x: Pandas DataFrame Each column represents a cell in the factorial design. For example, a 2x3 design would correspond to a DataFrame with 6 columns (levels of Factor A x levels of Factor B). Order your columns according to the following pattern (traversing each row in a matrix): the first column contains data for level 1 of Factor A and level 1 of Factor B the second column contains data for level 1 of Factor A and level 2 of Factor B column K contains the data for level 1 of Factor A and level K of Factor B column K + 1 contains the data for level 2 of Factor A and level 1 of Factor B and so on ... tr: float Proportion to trim (default is .2) con: array con is a K by d (number of contrasts) matrix containing the contrast coefficents of interest. All linear constrasts can be created automatically by using the function con1way (the result of which can be used for con ). alpha: float Alpha level (default is .05) dif: bool When True , use difference scores, otherwise use marginal distributions pool: bool If True , pool dependent groups together (default is False ). Otherwise generate pairwise contrasts across factor A for each level of factor B. hoch: bool When True , Hochberg's sequentially rejective method can be used to control FWE Return: Dictonary of results Try this example yourself in Colab! bwmcp \u00b6 bwmcp ( J , K , x , alpha =. 05 , tr =. 2 , nboot = 599 , seed = False ) A bootstrap-t for multiple comparisons among for all main effects and interactions in a between-by-within design. The analysis is done by generating bootstrap samples and using an appropriate linear contrast. Parameters: J: int Number of J levels associated with Factor A K: int Number of K levels associated with Factor B x: Pandas DataFrame Each column represents a cell in the factorial design. For example, a 2x3 design would correspond to a DataFrame with 6 columns (levels of Factor A x levels of Factor B). Order your columns according to the following pattern (traversing each row in a matrix): the first column contains data for level 1 of Factor A and level 1 of Factor B the second column contains data for level 1 of Factor A and level 2 of Factor B column K contains the data for level 1 of Factor A and level K of Factor B column K + 1 contains the data for level 2 of Factor A and level 1 of Factor B and so on ... alpha: float Alpha level (default is .05) tr: float Proportion to trim (default is .2) nboot: int Number of bootstrap samples (default is 500) seed: bool Random seed for reprodicible results (default is False ) Return: Dictonary of results Try this example yourself in Colab! bwimcp \u00b6 bwimcp ( J , K , x , tr =. 2 , alpha =. 05 ) Multiple comparisons for interactions in a split-plot design. The analysis is done by taking difference scores among all pairs of dependent groups and determining which of these differences differ across levels of Factor A using trimmed means. FWE is controlled via Hochberg's method. For MOM or M-estimators (possibly not implemented yet), use spmcpi which uses a bootstrap method Parameters: J: int Number of J levels associated with Factor A K: int Number of K levels associated with Factor B x: Pandas DataFrame Each column represents a cell in the factorial design. For example, a 2x3 design would correspond to a DataFrame with 6 columns (levels of Factor A x levels of Factor B). Order your columns according to the following pattern (traversing each row in a matrix): the first column contains data for level 1 of Factor A and level 1 of Factor B the second column contains data for level 1 of Factor A and level 2 of Factor B column K contains the data for level 1 of Factor A and level K of Factor B column K + 1 contains the data for level 2 of Factor A and level 1 of Factor B and so on ... tr: float Proportion to trim (default is .2) alpha: float Alpha level (default is .05) Return: Dictonary of results Try this example yourself in Colab! bwmcppb \u00b6 bwmcppb ( J , K , x , est , * args , alpha =. 05 , nboot = 500 , bhop = True , seed = True ) (note: this is for trimmed means only depite the est arg. This will be fixed eventually. Use trim_mean from SciPy) A percentile bootstrap for multiple comparisons for all main effects and interactions The analysis is done by generating bootstrap samples and using an appropriate linear contrast. Uses Rom's method to control FWE. Setting the argument bhop to True uses the Benjamini\u2013Hochberg method instead. Parameters: J: int Number of J levels associated with Factor A K: int Number of K levels associated with Factor B x: Pandas DataFrame Each column represents a cell in the factorial design. For example, a 2x3 design would correspond to a DataFrame with 6 columns (levels of Factor A x levels of Factor B). Order your columns according to the following pattern (traversing each row in a matrix): the first column contains data for level 1 of Factor A and level 1 of Factor B the second column contains data for level 1 of Factor A and level 2 of Factor B column K contains the data for level 1 of Factor A and level K of Factor B column K + 1 contains the data for level 2 of Factor A and level 1 of Factor B and so on ... est: function Measure of location (currently only trim_mean is supported) *args: list/value Parameter(s) for measure of location (e.g., .2) alpha: float Alpha level. Default is .05. nboot: int Number of bootstrap samples (default is 500) bhop: bool When True , use the Benjamini\u2013Hochberg method to control FWE seed: bool Random seed for reprodicible results (default is False ) Return: Dictonary of results Try this example yourself in Colab! spmcpa \u00b6 spmcpa ( J , K , x , est , * args , avg = False , alpha =. 05 , nboot = None , seed = False ) All pairwise comparisons among levels of Factor A in a mixed design. A sequentially rejective method is used to control FWE. The avg option controls whether or not to average data across levels of Factor B prior to performing the statistical test. If False , contrasts are created to test across Factor A for each level of Factor B. Parameters: x: Pandas DataFrame Data for group one est: function Measure of location (currently only trim_mean is supported) *args: list/value Parameter(s) for measure of location (e.g., .2) avg: bool If False , contrasts are created to test across Factor A for each level of Factor B (default is False ) alpha: float Alpha level (default is .05) nboot: int Number of bootstrap samples (default is None in which case the number is chosen based on the number of contrasts). seed: bool Random seed for reprodicible results (default is False ) Return: Dictonary of results Try this example yourself in Colab! spmcpb \u00b6 spmcpb ( J , K , x , est , * args , dif = True , alpha =. 05 , nboot = 599 , seed = False ) All pairwise comparisons among levels of Factor B in a split-plot design. A sequentially rejective method is used to control FWE. If est is onestep or mom (not be implemeted yet), method SR is used to control the probability of at least one Type I error. Otherwise, Hochberg is used. Parameters: x: Pandas DataFrame Data for group one est: function Measure of location (currently only trim_mean is supported) *args: list/value Parameter(s) for measure of location (e.g., .2) dif: bool When True , use difference scores, otherwise use marginal distributions alpha: float Alpha level (default is .05) nboot: int Number of bootstrap samples (default is 599) seed: bool Random seed for reprodicible results (default is False ) Return: Dictonary of results Try this example yourself in Colab! spmcpi \u00b6 spmcpi ( J , K , x , est , * args , alpha =. 05 , nboot = None , SR = False , seed = False ) Multiple comparisons for interactions in a split-plot design. The analysis is done by taking difference scores among all pairs of dependent groups and determining which of these differences differ across levels of Factor A. The so-called the SR method, which is a slight modification of Hochberg's (1988) \"sequentially rejective\" method can be applied to control FWE, especially when comparing one-step M-estimators or M-estimators. Parameters: x: Pandas DataFrame Data for group one est: function Measure of location (currently only trim_mean is supported) *args: list/value Parameter(s) for measure of location (e.g., .2) alpha: float Alpha level. Default is .05. nboot: int Number of bootstrap samples (default is None in which case the number is chosen based on the number of contrasts) SR: bool When True , use the slight modification of Hochberg's (1988) \"sequentially rejective\" method to control FWE seed: bool Random seed for reprodicible results (default is False ) Return: Dictonary of results Try this example yourself in Colab! Measuring associations \u00b6 For statistical tests and measurements that include robust correlations and tests of independence. Note that regression functions will be added here eventually. corb \u00b6 corb ( corfun , x , y , alpha , nboot , * args , seed = False ) Compute a 1-alpha confidence interval for a correlation using percentile bootstrap method The function corfun is any function that returns a correlation coefficient. The functions pbcor and wincor follow this convention. When using Pearson's correlation, and when n<250, use lsfitci instead (not yet implemented). Parameters: corfun: function corfun is any function that returns a correlation coefficient x: Pandas Series Data for group one y: Pandas Series Data for group two alpha: float Alpha level (default is .05) nboot: int Number of bootstrap samples *args: list/value List of arguments to corfun (e.g., .2) seed: bool Random seed for reprodicible results. Default is False . Return: List containing CI, p_value, and correlation estimate Try this example yourself in Colab! pball \u00b6 pball ( x , beta =. 2 ) Compute the percentage bend correlation matrix for all pairs of columns in x . This function also returns the two-sided significance level for all pairs of variables, plus a test of zero correlation among all pairs. Parameters: x: Pandas DataFrame Each column represents a variable to use in the correlations beta: float 0 < beta < .5 . Beta is analogous to trimming in other functions and related to the measure of dispersion used in the percentage bend calculation. Return: Dictonary of results Try this example yourself in Colab! pbcor \u00b6 pbcor ( x , y , beta =. 2 ) Compute the percentage bend correlation between x and y Parameters: x: Pandas Series Data for group one y: Pandas Series Data for group two beta: float 0 < beta < .5 . Beta is analogous to trimming in other functions and related to the measure of dispersion used in the percentage bend calculation. Return: Dictonary of results Try this example yourself in Colab! winall \u00b6 winall ( x , tr =. 2 ) Compute the Winsorized correlation and covariance matrix for all pairs of columns in x . This function also returns the two-sided significance level for all pairs of variables, plus a test of zero correlation among all pairs. Parameters: x: Pandas DataFrame Each column represents a variable to use in the correlations tr: float Proportion to winsorize (default is .2) Return: Dictonary of results Try this example yourself in Colab! wincor \u00b6 wincor ( x , y , tr =. 2 ) Compute the winsorized correlation between x and y . This function also returns the winsorized covariance. Parameters: x: Pandas Series Data for group one y: Pandas Series Data for group two tr: float Proportion to winsorize (default is .2) Return: Dictonary of results Try this example yourself in Colab!","title":"Function Reference"},{"location":"function_guide/#function-reference","text":"Hypothesize exposes the following top-level functions for comparing groups and measuring associations. The function names, code, and descriptions are kept generally consistent with Wilcox's WRS package. If you want to learn more about the theory and research behind any given function here, see Wilcox's books, especially Introduction to Robust Estimation and Hypothesis Testing . Jump to: Comparing groups with a single factor Comparing groups with two factors Measuring associations","title":"Function Reference"},{"location":"function_guide/#comparing-groups-with-a-single-factor","text":"Statistical tests analogous to a 1-way ANOVA or T-tests. That is, group tests that have a single factor.","title":"Comparing groups with a single factor"},{"location":"function_guide/#independent-groups","text":"","title":"Independent groups"},{"location":"function_guide/#l2drmci","text":"l2drmci ( x , y , est , * args , pairwise_drop_na = True , alpha =. 05 , nboot = 2000 , seed = False ) Compute a bootstrap confidence interval for a measure of location associated with the distribution of x-y. That is, compare x and y by looking at all possible difference scores in random samples of x and y . x and y are possibly dependent. Parameters: x: Pandas Series Data for group one y: Pandas Series Data for group two est: function Measure of location (currently only trim_mean is supported) *args: list/value Parameter(s) for measure of location (e.g., .2) pairwise_drop_na: bool If True, treat data as dependent and remove any row with missing data. If False, remove missing data for each group seperately (cannot deal with unequal sample sizes) alpha: float Alpha level (default is .05) nboot: int Number of bootstrap samples (default is 2000) seed: bool Random seed for reprodicible results (default is False ) Return: Dictonary of results Try this example yourself in Colab!","title":"l2drmci"},{"location":"function_guide/#linconb","text":"linconb ( x , con , tr =. 2 , alpha =. 05 , nboot = 599 , seed = False ) Compute a 1-alpha confidence interval for a set of d linear contrasts involving trimmed means using the bootstrap-t bootstrap method. Independent groups are assumed. CIs are adjusted to control FWE (p values are not adjusted). Parameters: x: DataFrame Each column represents a group of data con: array con is a J (number of columns) by d (number of contrasts) matrix containing the contrast coefficents of interest. All linear constrasts can be created automatically by using the function con1way (the result of which can be used for con ). tr: float Proportion to trim (default is .2) alpha: float Alpha level (default is .05) nboot: int Number of bootstrap samples (default is 2000) seed: bool Random seed for reprodicible results. Default is False . Return: Dictonary of results Try this example yourself in Colab!","title":"linconb"},{"location":"function_guide/#pb2gen","text":"pb2gen ( x , y , est , * args , alpha =. 05 , nboot = 2000 , seed = False ) Compute a bootstrap confidence interval for the the difference between any two parameters corresponding to two independent groups. Parameters: x: Pandas Series Data for group one y: Pandas Series Data for group two est: function Measure of location (currently only trim_mean is supported) *args: list/value Parameter(s) for measure of location (e.g., .2) alpha: float Alpha level (default is .05) nboot: int Number of bootstrap samples (default is 2000) seed: bool Random seed for reprodicible results (default is False ) Return: Dictonary of results Try this example yourself in Colab!","title":"pb2gen"},{"location":"function_guide/#tmcppb","text":"tmcppb ( x , est , * args , con = None , bhop = False , alpha =. 05 , nboot = None , seed = False ) Multiple comparisons for J independent groups using trimmed means and the percentile bootstrap method. Rom\u2019s method is used to control the probability of one or more type I errors. For C > 10 hypotheses, or when the goal is to test at some level other than .05 and .01, Hochberg\u2019s method is used. Setting the argument bhop to True uses the Benjamini\u2013Hochberg method instead. Parameters: x: Pandas DataFrame Each column represents a group of data est: function Measure of location (currently only trim_mean is supported) *args: list/value Parameter(s) for measure of location (e.g., .2) con: array con is a J (number of columns) by d (number of contrasts) matrix containing the contrast coefficents of interest. All linear constrasts can be created automatically by using the function con1way (the result of which can be used for con ). The default is None and in this case all linear contrasts are created automatically. bhop: bool If True , the Benjamini\u2013Hochberg method is used to control FWE alpha: float Alpha level. Default is .05. nboot: int Number of bootstrap samples (default is 2000) seed: bool Random seed for reproducible results. Default is False . Return: Dictonary of results Try this example yourself in Colab!","title":"tmcppb"},{"location":"function_guide/#yuenbt","text":"yuenbt ( x , y , tr =. 2 , alpha =. 05 , nboot = 599 , seed = False ) Compute a 1-alpha confidence interval for the difference between the trimmed means corresponding to two independent groups. The bootstrap-t method is used. During the bootstrapping, the absolute value of the test statistic is used (the \"two-sided method\"). Parameters: x: Pandas Series Data for group one y: Pandas Series Data for group two tr: float Proportion to trim (default is .2) alpha: float Alpha level (default is .05) nboot: int Number of bootstrap samples (default is 599) seed: bool Random seed for reprodicible results. Default is False . Return: Dictonary of results Try this example yourself in Colab!","title":"yuenbt"},{"location":"function_guide/#dependent-groups","text":"","title":"Dependent groups"},{"location":"function_guide/#bootdpci","text":"bootdpci ( x , est , * args , nboot = None , alpha =. 05 , dif = True , BA = False , SR = False ) Use percentile bootstrap method, compute a .95 confidence interval for the difference between a measure of location or scale when comparing two dependent groups. The argument dif defaults to True indicating that difference scores will be used, in which case Hochberg\u2019s method is used to control FWE. If dif is False , measures of location associated with the marginal distributions are used instead. If dif is False and BA is True , the bias adjusted estimate of the generalized p-value is recommended. Using BA = True (when dif = False ) is recommended when comparing groups with M-estimators and MOM, but it is not necessary when comparing 20% trimmed means (Wilcox & Keselman, 2002). The so-called the SR method, which is a slight modification of Hochberg's (1988) \"sequentially rejective\" method can be applied to control FWE, especially when comparing one-step M-estimators or M-estimators. Parameters: x: Pandas DataFrame Each column represents a group of data est: function Measure of location (currently only trim_mean is supported) *args: list/value Parameter(s) for measure of location (e.g., .2) alpha: float Alpha level. Default is .05. nboot: int Number of bootstrap samples. Default is None in which case nboot will be chosen for you based on the number of contrasts. dif: bool When True , use difference scores, otherwise use marginal distributions BA: bool When True , use the bias adjusted estimate of the generalized p-value is applied (e.g., when dif is False ) SR: bool When True , use the modified \"sequentially rejective\", especially when comparing one-step M-estimators or M-estimators. Return: Dictonary of results Try this example yourself in Colab!","title":"bootdpci"},{"location":"function_guide/#rmmcppb","text":"rmmcppb ( x , est , * args , alpha =. 05 , con = None , dif = True , nboot = None , BA = False , hoch = False , SR = False , seed = False ) Use a percentile bootstrap method to compare dependent groups. By default, compute a .95 confidence interval for all linear contrasts specified by con, a J-by-C matrix, where C is the number of contrasts to be tested, and the columns of con are the contrast coefficients. If con is not specified, all pairwise comparisons are done. If est is the function onestep or mom (these are not implemeted yet), method SR can be used to control the probability of at least one Type I error. Otherwise, Hochberg's method is used. If dif is False and BA is True , the bias adjusted estimate of the generalized p-value is recommended. Using BA = True (when dif = False ) is recommended when comparing groups with M-estimators and MOM, but it is not necessary when comparing 20% trimmed means (Wilcox & Keselman, 2002). Hochberg's sequentially rejective method can be used and is used if n>=80. Parameters: x: Pandas DataFrame Each column represents a group of data est: function Measure of location (currently only trim_mean is supported) *args: list/value Parameter(s) for measure of location (e.g., .2) alpha: float Alpha level (default is .05) con: array con is a J (number of columns) by d (number of contrasts) matrix containing the contrast coefficents of interest. All linear constrasts can be created automatically by using the function con1way (the result of which can be used for con ). The default is None and in this case all linear contrasts are created automatically. dif: bool When True , use difference scores, otherwise use marginal distributions nboot: int Number of bootstrap samples. Default is None in which case nboot will be chosen for you based on the number of contrasts. BA: bool When True , use the bias adjusted estimate of the generalized p-value is applied (e.g., when dif is False ) hoch: bool When True , Hochberg's sequentially rejective method can be used and is used if n>=80. SR: bool When True , use the modified \"sequentially rejective\", especially when comparing one-step M-estimators or M-estimators. seed: bool Random seed for reprodicible results (default is False ) Return: Dictonary of results Try this example yourself in Colab!","title":"rmmcppb"},{"location":"function_guide/#lindepbt","text":"lindepbt ( x , tr =. 2 , con = None , alpha =. 05 , nboot = 599 , dif = True , seed = False ) Multiple comparisons on trimmed means with FWE controlled with Rom's method Using a bootstrap-t method. Parameters: x: Pandas DataFrame Each column in the data represents a different group tr: float Proportion to trim (default is .2) con: array con is a J (number of groups) by d (number of contrasts) matrix containing the contrast coefficents of interest. All linear constrasts can be created automatically by using the function con1way (the result of which can be used for con ). The default is None and in this case all linear contrasts are created automatically. alpha: float Alpha level. Default is .05. nboot: int Number of bootstrap samples (default is 2000) dif: bool When True , use difference scores, otherwise use marginal distributions seed: bool Random seed for reprodicible results (default is False ) Return: Dictonary of results Try this example yourself in Colab!","title":"lindepbt"},{"location":"function_guide/#ydbt","text":"ydbt ( x , y , tr =. 2 , alpha =. 05 , nboot = 599 , side = True , seed = False ) Using the bootstrap-t method, compute a .95 confidence interval for the difference between the marginal trimmed means of paired data. By default, 20% trimming is used with 599 bootstrap samples. Parameters: x: Pandas Series Data for group one y: Pandas Series Data for group two tr: float Proportion to trim (default is .2) alpha: float Alpha level. Default is .05. nboot: int Number of bootstrap samples (default is 2000) side: bool When True the function returns a symmetric CI and a p value, otherwise the function returns equal-tailed CI (no p value) seed: bool Random seed for reprodicible results (default is False ) Return: Dictonary of results Try this example yourself in Colab!","title":"ydbt"},{"location":"function_guide/#comparing-groups-with-two-factors","text":"Statistical tests analogous to a 2-way ANOVA. That is, group tests that have a two factors.","title":"Comparing groups with two factors"},{"location":"function_guide/#dependent-groups_1","text":"","title":"Dependent groups"},{"location":"function_guide/#wwmcppb","text":"wwmcppb ( J , K , x , est , * args , alpha =. 05 , dif = True , nboot = None , BA = True , hoch = True , seed = False ) Do all multiple comparisons for a within-by-within design using a percentile bootstrap method.A sequentially rejective method is used to control alpha. Hochberg's method can be used and is if n>=80. Parameters: J: int Number of J levels associated with Factor A K: int Number of K levels associated with Factor B x: Pandas DataFrame Each column represents a cell in the factorial design. For example, a 2x3 design would correspond to a DataFrame with 6 columns (levels of Factor A x levels of Factor B). Order your columns according to the following pattern (traversing each row in a matrix): the first column contains data for level 1 of Factor A and level 1 of Factor B the second column contains data for level 1 of Factor A and level 2 of Factor B column K contains the data for level 1 of Factor A and level K of Factor B column K + 1 contains the data for level 2 of Factor A and level 1 of Factor B and so on ... est: function Measure of location (currently only trim_mean is supported) *args: list/value Parameter(s) for measure of location (e.g., .2) alpha: float Alpha level (default is .05) dif: bool When True , use difference scores, otherwise use marginal distributions nboot: int Number of bootstrap samples (default is 599) BA: bool When True , use the bias adjusted estimate of the generalized p-value is applied (e.g., when dif is False ) hoch: bool When True , Hochberg's sequentially rejective method can be used to control FWE seed: bool Random seed for reprodicible results (default is False ) Return: Dictonary of results Try this example yourself in Colab!","title":"wwmcppb"},{"location":"function_guide/#wwmcpbt","text":"wwmcpbt ( J , K , x , tr =. 2 , alpha =. 05 , nboot = 599 , seed = False ) Do multiple comparisons for a within-by-within design. using a bootstrap-t method and trimmed means. All linear contrasts relevant to main effects and interactions are tested. With trimmed means FWE is controlled with Rom's method. (currently unsure whether or not dif should be here) Parameters: J: int Number of J levels associated with Factor A K: int Number of K levels associated with Factor B x: Pandas DataFrame Each column represents a cell in the factorial design. For example, a 2x3 design would correspond to a DataFrame with 6 columns (levels of Factor A x levels of Factor B). Order your columns according to the following pattern (traversing each row in a matrix): the first column contains data for level 1 of Factor A and level 1 of Factor B the second column contains data for level 1 of Factor A and level 2 of Factor B column K contains the data for level 1 of Factor A and level K of Factor B column K + 1 contains the data for level 2 of Factor A and level 1 of Factor B and so on ... tr: float Proportion to trim (default is .2) alpha: float Alpha level (default is .05) nboot: int Number of bootstrap samples (default is 599) seed: bool Random seed for reprodicible results (default is False ) Return: Dictonary of results Try this example yourself in Colab!","title":"wwmcpbt"},{"location":"function_guide/#mixed-designs","text":"These designs are also known as \"split-plot\" or \"between-within\" desgins. Hypothesize follws the common convention that assumes that the between-subjects factor is factor A and the within-subjects conditions are Factor B. For example, in a 2x3 mixed design, factor A has two levels. For each of these levels, there are 3 within-subjects conditions. Make sure your DataFrame corresponds to a between-within design, not the other way around For example, in a 2x3 mixed design, the first 3 columns correspond to the first level of Factor A. The last 3 columns correspond to the second level of factor A.","title":"Mixed designs"},{"location":"function_guide/#bwamcp","text":"bwamcp ( J , K , x , tr =. 2 , alpha =. 05 , pool = False ) All pairwise comparisons among levels of Factor A in a mixed design using trimmed means. The pool option allows you to pool dependent groups across Factor A for each level of Factor B. Parameters: J: int Number of J levels associated with Factor A K: int Number of K levels associated with Factor B x: Pandas DataFrame Each column represents a cell in the factorial design. For example, a 2x3 design would correspond to a DataFrame with 6 columns (levels of Factor A x levels of Factor B). Order your columns according to the following pattern (traversing each row in a matrix): the first column contains data for level 1 of Factor A and level 1 of Factor B the second column contains data for level 1 of Factor A and level 2 of Factor B column K contains the data for level 1 of Factor A and level K of Factor B column K + 1 contains the data for level 2 of Factor A and level 1 of Factor B and so on ... tr: float Proportion to trim (default is .2) alpha: float Alpha level (default is .05) pool: bool If True , pool dependent groups together (default is False ). Otherwise generate pairwise contrasts across factor A for each level of factor B. Return: Dictonary of results Try this example yourself in Colab!","title":"bwamcp"},{"location":"function_guide/#bwbmcp","text":"bwbmcp ( J , K , x , tr =. 2 , con = None , alpha =. 05 , dif = True , pool = False , hoch = False ) All pairwise comparisons among levels of Factor B in a mixed design using trimmed means. The pool option allows you to pool dependent groups across Factor A for each level of Factor B. Rom's method is used to control for FWE (when alpha is 0.5, .01, or when number of comparisons are > 10). Hochberg's method can also be used. Note that CIs are adjusted based on the corresponding critical p-value after controling for FWE. Parameters: J: int Number of J levels associated with Factor A K: int Number of K levels associated with Factor B x: Pandas DataFrame Each column represents a cell in the factorial design. For example, a 2x3 design would correspond to a DataFrame with 6 columns (levels of Factor A x levels of Factor B). Order your columns according to the following pattern (traversing each row in a matrix): the first column contains data for level 1 of Factor A and level 1 of Factor B the second column contains data for level 1 of Factor A and level 2 of Factor B column K contains the data for level 1 of Factor A and level K of Factor B column K + 1 contains the data for level 2 of Factor A and level 1 of Factor B and so on ... tr: float Proportion to trim (default is .2) con: array con is a K by d (number of contrasts) matrix containing the contrast coefficents of interest. All linear constrasts can be created automatically by using the function con1way (the result of which can be used for con ). alpha: float Alpha level (default is .05) dif: bool When True , use difference scores, otherwise use marginal distributions pool: bool If True , pool dependent groups together (default is False ). Otherwise generate pairwise contrasts across factor A for each level of factor B. hoch: bool When True , Hochberg's sequentially rejective method can be used to control FWE Return: Dictonary of results Try this example yourself in Colab!","title":"bwbmcp"},{"location":"function_guide/#bwmcp","text":"bwmcp ( J , K , x , alpha =. 05 , tr =. 2 , nboot = 599 , seed = False ) A bootstrap-t for multiple comparisons among for all main effects and interactions in a between-by-within design. The analysis is done by generating bootstrap samples and using an appropriate linear contrast. Parameters: J: int Number of J levels associated with Factor A K: int Number of K levels associated with Factor B x: Pandas DataFrame Each column represents a cell in the factorial design. For example, a 2x3 design would correspond to a DataFrame with 6 columns (levels of Factor A x levels of Factor B). Order your columns according to the following pattern (traversing each row in a matrix): the first column contains data for level 1 of Factor A and level 1 of Factor B the second column contains data for level 1 of Factor A and level 2 of Factor B column K contains the data for level 1 of Factor A and level K of Factor B column K + 1 contains the data for level 2 of Factor A and level 1 of Factor B and so on ... alpha: float Alpha level (default is .05) tr: float Proportion to trim (default is .2) nboot: int Number of bootstrap samples (default is 500) seed: bool Random seed for reprodicible results (default is False ) Return: Dictonary of results Try this example yourself in Colab!","title":"bwmcp"},{"location":"function_guide/#bwimcp","text":"bwimcp ( J , K , x , tr =. 2 , alpha =. 05 ) Multiple comparisons for interactions in a split-plot design. The analysis is done by taking difference scores among all pairs of dependent groups and determining which of these differences differ across levels of Factor A using trimmed means. FWE is controlled via Hochberg's method. For MOM or M-estimators (possibly not implemented yet), use spmcpi which uses a bootstrap method Parameters: J: int Number of J levels associated with Factor A K: int Number of K levels associated with Factor B x: Pandas DataFrame Each column represents a cell in the factorial design. For example, a 2x3 design would correspond to a DataFrame with 6 columns (levels of Factor A x levels of Factor B). Order your columns according to the following pattern (traversing each row in a matrix): the first column contains data for level 1 of Factor A and level 1 of Factor B the second column contains data for level 1 of Factor A and level 2 of Factor B column K contains the data for level 1 of Factor A and level K of Factor B column K + 1 contains the data for level 2 of Factor A and level 1 of Factor B and so on ... tr: float Proportion to trim (default is .2) alpha: float Alpha level (default is .05) Return: Dictonary of results Try this example yourself in Colab!","title":"bwimcp"},{"location":"function_guide/#bwmcppb","text":"bwmcppb ( J , K , x , est , * args , alpha =. 05 , nboot = 500 , bhop = True , seed = True ) (note: this is for trimmed means only depite the est arg. This will be fixed eventually. Use trim_mean from SciPy) A percentile bootstrap for multiple comparisons for all main effects and interactions The analysis is done by generating bootstrap samples and using an appropriate linear contrast. Uses Rom's method to control FWE. Setting the argument bhop to True uses the Benjamini\u2013Hochberg method instead. Parameters: J: int Number of J levels associated with Factor A K: int Number of K levels associated with Factor B x: Pandas DataFrame Each column represents a cell in the factorial design. For example, a 2x3 design would correspond to a DataFrame with 6 columns (levels of Factor A x levels of Factor B). Order your columns according to the following pattern (traversing each row in a matrix): the first column contains data for level 1 of Factor A and level 1 of Factor B the second column contains data for level 1 of Factor A and level 2 of Factor B column K contains the data for level 1 of Factor A and level K of Factor B column K + 1 contains the data for level 2 of Factor A and level 1 of Factor B and so on ... est: function Measure of location (currently only trim_mean is supported) *args: list/value Parameter(s) for measure of location (e.g., .2) alpha: float Alpha level. Default is .05. nboot: int Number of bootstrap samples (default is 500) bhop: bool When True , use the Benjamini\u2013Hochberg method to control FWE seed: bool Random seed for reprodicible results (default is False ) Return: Dictonary of results Try this example yourself in Colab!","title":"bwmcppb"},{"location":"function_guide/#spmcpa","text":"spmcpa ( J , K , x , est , * args , avg = False , alpha =. 05 , nboot = None , seed = False ) All pairwise comparisons among levels of Factor A in a mixed design. A sequentially rejective method is used to control FWE. The avg option controls whether or not to average data across levels of Factor B prior to performing the statistical test. If False , contrasts are created to test across Factor A for each level of Factor B. Parameters: x: Pandas DataFrame Data for group one est: function Measure of location (currently only trim_mean is supported) *args: list/value Parameter(s) for measure of location (e.g., .2) avg: bool If False , contrasts are created to test across Factor A for each level of Factor B (default is False ) alpha: float Alpha level (default is .05) nboot: int Number of bootstrap samples (default is None in which case the number is chosen based on the number of contrasts). seed: bool Random seed for reprodicible results (default is False ) Return: Dictonary of results Try this example yourself in Colab!","title":"spmcpa"},{"location":"function_guide/#spmcpb","text":"spmcpb ( J , K , x , est , * args , dif = True , alpha =. 05 , nboot = 599 , seed = False ) All pairwise comparisons among levels of Factor B in a split-plot design. A sequentially rejective method is used to control FWE. If est is onestep or mom (not be implemeted yet), method SR is used to control the probability of at least one Type I error. Otherwise, Hochberg is used. Parameters: x: Pandas DataFrame Data for group one est: function Measure of location (currently only trim_mean is supported) *args: list/value Parameter(s) for measure of location (e.g., .2) dif: bool When True , use difference scores, otherwise use marginal distributions alpha: float Alpha level (default is .05) nboot: int Number of bootstrap samples (default is 599) seed: bool Random seed for reprodicible results (default is False ) Return: Dictonary of results Try this example yourself in Colab!","title":"spmcpb"},{"location":"function_guide/#spmcpi","text":"spmcpi ( J , K , x , est , * args , alpha =. 05 , nboot = None , SR = False , seed = False ) Multiple comparisons for interactions in a split-plot design. The analysis is done by taking difference scores among all pairs of dependent groups and determining which of these differences differ across levels of Factor A. The so-called the SR method, which is a slight modification of Hochberg's (1988) \"sequentially rejective\" method can be applied to control FWE, especially when comparing one-step M-estimators or M-estimators. Parameters: x: Pandas DataFrame Data for group one est: function Measure of location (currently only trim_mean is supported) *args: list/value Parameter(s) for measure of location (e.g., .2) alpha: float Alpha level. Default is .05. nboot: int Number of bootstrap samples (default is None in which case the number is chosen based on the number of contrasts) SR: bool When True , use the slight modification of Hochberg's (1988) \"sequentially rejective\" method to control FWE seed: bool Random seed for reprodicible results (default is False ) Return: Dictonary of results Try this example yourself in Colab!","title":"spmcpi"},{"location":"function_guide/#measuring-associations","text":"For statistical tests and measurements that include robust correlations and tests of independence. Note that regression functions will be added here eventually.","title":"Measuring associations"},{"location":"function_guide/#corb","text":"corb ( corfun , x , y , alpha , nboot , * args , seed = False ) Compute a 1-alpha confidence interval for a correlation using percentile bootstrap method The function corfun is any function that returns a correlation coefficient. The functions pbcor and wincor follow this convention. When using Pearson's correlation, and when n<250, use lsfitci instead (not yet implemented). Parameters: corfun: function corfun is any function that returns a correlation coefficient x: Pandas Series Data for group one y: Pandas Series Data for group two alpha: float Alpha level (default is .05) nboot: int Number of bootstrap samples *args: list/value List of arguments to corfun (e.g., .2) seed: bool Random seed for reprodicible results. Default is False . Return: List containing CI, p_value, and correlation estimate Try this example yourself in Colab!","title":"corb"},{"location":"function_guide/#pball","text":"pball ( x , beta =. 2 ) Compute the percentage bend correlation matrix for all pairs of columns in x . This function also returns the two-sided significance level for all pairs of variables, plus a test of zero correlation among all pairs. Parameters: x: Pandas DataFrame Each column represents a variable to use in the correlations beta: float 0 < beta < .5 . Beta is analogous to trimming in other functions and related to the measure of dispersion used in the percentage bend calculation. Return: Dictonary of results Try this example yourself in Colab!","title":"pball"},{"location":"function_guide/#pbcor","text":"pbcor ( x , y , beta =. 2 ) Compute the percentage bend correlation between x and y Parameters: x: Pandas Series Data for group one y: Pandas Series Data for group two beta: float 0 < beta < .5 . Beta is analogous to trimming in other functions and related to the measure of dispersion used in the percentage bend calculation. Return: Dictonary of results Try this example yourself in Colab!","title":"pbcor"},{"location":"function_guide/#winall","text":"winall ( x , tr =. 2 ) Compute the Winsorized correlation and covariance matrix for all pairs of columns in x . This function also returns the two-sided significance level for all pairs of variables, plus a test of zero correlation among all pairs. Parameters: x: Pandas DataFrame Each column represents a variable to use in the correlations tr: float Proportion to winsorize (default is .2) Return: Dictonary of results Try this example yourself in Colab!","title":"winall"},{"location":"function_guide/#wincor","text":"wincor ( x , y , tr =. 2 ) Compute the winsorized correlation between x and y . This function also returns the winsorized covariance. Parameters: x: Pandas Series Data for group one y: Pandas Series Data for group two tr: float Proportion to winsorize (default is .2) Return: Dictonary of results Try this example yourself in Colab!","title":"wincor"},{"location":"install_dep/","text":"Installation \u00b6 Hypothesize can be installed using pip : $ pip install hypothesize Dependencies \u00b6 Hypothesesize has the following dependencies, all of which are installed automatically with the above installation command: python 3.6 or newer NumPy Pandas SciPy more-itertools","title":"Installation"},{"location":"install_dep/#installation","text":"Hypothesize can be installed using pip : $ pip install hypothesize","title":"Installation"},{"location":"install_dep/#dependencies","text":"Hypothesesize has the following dependencies, all of which are installed automatically with the above installation command: python 3.6 or newer NumPy Pandas SciPy more-itertools","title":"Dependencies"},{"location":"junk/","text":"doc tools \u00b6 inline highlight bootdpci ( x , est , * args , nboot = None , alpha =. 05 , dif = True , BA = False , SR = False ) tabbed list Fruit List :apple: Apple :banana: Banana :kiwi: Kiwi Fruit Table Fruit Color :apple: Apple Red :banana: Banana Yellow :kiwi: Kiwi Green alerts A simple primary alert\u2014check it out! A simple info alert\u2014check it out! code def my_func (): i = 1 print ( 'hello_world' ) admonition optional explicit title within double quotes Any number of other indented markdown elements. def my_func(hello='world'): for item in my_list: print(item, 12) button this is a link-button this is a block link-button","title":"Junk"},{"location":"junk/#doc-tools","text":"inline highlight bootdpci ( x , est , * args , nboot = None , alpha =. 05 , dif = True , BA = False , SR = False ) tabbed list Fruit List :apple: Apple :banana: Banana :kiwi: Kiwi Fruit Table Fruit Color :apple: Apple Red :banana: Banana Yellow :kiwi: Kiwi Green alerts A simple primary alert\u2014check it out! A simple info alert\u2014check it out! code def my_func (): i = 1 print ( 'hello_world' ) admonition optional explicit title within double quotes Any number of other indented markdown elements. def my_func(hello='world'): for item in my_list: print(item, 12) button this is a link-button this is a block link-button","title":"doc tools"},{"location":"overview/","text":"Overview \u00b6 The benefits of using robust methods for hypothesis testing have been known for the last half century. They have been shown to subtantially increase power and accuracy when compared to traditional approaches. The issues of robustness and the functions in this library are described in detail in Rand R. Wilcox's book Introduction to Robust Estimation and Hypothesis Testing . The code and function names in Hypothesize are based on Wilcox's R functions in the WRS package. Hypothesize simply brings many of these helpful and well-studied robust methods to the Python ecosystem. In addition, Hypothesize provides a user-friendly API and package structure as well as one-click, ready-to-run examples for every top-level function. Hypothesize is easy to use \u00b6 Hypothesize's API is friendly and consistent, making it easy for you to discover and use robust functions that are appropriate for your statistical design. Package Structure \u00b6 Hypothesize organizes functions based on the statistical design. The following visualizations show how the package is structured and how this is reflected in practice when importing from the library: graph TB linkStyle default interpolate basis A[Hypothesize] A --> B(compare groups with single factor) A --> C(compare groups with two factors) A --> D(measure associations) B --> F(f<sub>1</sub>) B --> G(f<sub>2</sub>) B --- H(f<sub>n</sub>) C --> F1(f<sub>1</sub>) C --> G2(f<sub>2</sub>) C --> H3(f<sub>n</sub>) D --> F5(f<sub>1</sub>) D --> G6(f<sub>2</sub>) D --> H7(f<sub>n</sub>) Hypothesize is flexible and powerful \u00b6 Depending on the statistical test, Hypothesize allows you to specify parameters to control the following (not an exhaustive list): the estimator trimmed mean winsorized correlation percentage bend correlation mean median Hypothesize focuses on methods that relate to the trimmed mean However, in places where Hypothesize accepts a function as an input, there is a good chance that other estimators will also work (e.g., one-step M-estimator, MOM) family-wise error (FWE) using sequentially rejective methods how group differences are calculated (e.g., marginal, pairwise, all combinations of differences) linear contrasts the alpha level the random seed (for reprodicible results when using bootstrap-based tests) Visit the tutorial section and the function documentation for complete examples using Hypothesize.","title":"Overview"},{"location":"overview/#overview","text":"The benefits of using robust methods for hypothesis testing have been known for the last half century. They have been shown to subtantially increase power and accuracy when compared to traditional approaches. The issues of robustness and the functions in this library are described in detail in Rand R. Wilcox's book Introduction to Robust Estimation and Hypothesis Testing . The code and function names in Hypothesize are based on Wilcox's R functions in the WRS package. Hypothesize simply brings many of these helpful and well-studied robust methods to the Python ecosystem. In addition, Hypothesize provides a user-friendly API and package structure as well as one-click, ready-to-run examples for every top-level function.","title":"Overview"},{"location":"overview/#hypothesize-is-easy-to-use","text":"Hypothesize's API is friendly and consistent, making it easy for you to discover and use robust functions that are appropriate for your statistical design.","title":"Hypothesize is easy to use"},{"location":"overview/#package-structure","text":"Hypothesize organizes functions based on the statistical design. The following visualizations show how the package is structured and how this is reflected in practice when importing from the library: graph TB linkStyle default interpolate basis A[Hypothesize] A --> B(compare groups with single factor) A --> C(compare groups with two factors) A --> D(measure associations) B --> F(f<sub>1</sub>) B --> G(f<sub>2</sub>) B --- H(f<sub>n</sub>) C --> F1(f<sub>1</sub>) C --> G2(f<sub>2</sub>) C --> H3(f<sub>n</sub>) D --> F5(f<sub>1</sub>) D --> G6(f<sub>2</sub>) D --> H7(f<sub>n</sub>)","title":"Package Structure"},{"location":"overview/#hypothesize-is-flexible-and-powerful","text":"Depending on the statistical test, Hypothesize allows you to specify parameters to control the following (not an exhaustive list): the estimator trimmed mean winsorized correlation percentage bend correlation mean median Hypothesize focuses on methods that relate to the trimmed mean However, in places where Hypothesize accepts a function as an input, there is a good chance that other estimators will also work (e.g., one-step M-estimator, MOM) family-wise error (FWE) using sequentially rejective methods how group differences are calculated (e.g., marginal, pairwise, all combinations of differences) linear contrasts the alpha level the random seed (for reprodicible results when using bootstrap-based tests) Visit the tutorial section and the function documentation for complete examples using Hypothesize.","title":"Hypothesize is flexible and powerful"}]}